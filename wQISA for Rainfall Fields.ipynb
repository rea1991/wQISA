{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weighted Quasi Interpolant Spline Approximation for Rainfall Fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We here provide a quantitative evaluation of a novel approach - the Weighted Quasi Interpolant Spline Approximation (w-QISA) -  in the approximation of observed rain data, in real conditions of sparsity of the observations. For a formal definition of w-QISA and its properties, see [1].\n",
    "\n",
    "This implementation is not optimized for high performance computing, but wants rather to be as intuitive as possible. This implementation rely on the library for tensor product and LR B-splines proposed at https://github.com/qTipTip/LRSplines, which has the advantage to be very simple and thus accessible to a larger public. For industrial grade performance and a more complete set of tools, see the [GoTools library](https://github.com/SINTEF-Geometry/GoTools) written in C++."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Liguria, observed rainfall data are captured by two different rain gauges networks. Liguria corresponds to a long and narrow strip of land, squeezed between the sea, the Alps and the Apennines mountains. The orography and the closeness to the sea make this area particularly interesting for hydro-meteorological events, frequently characterized by heavy rain due to Atlantic low pressure area, augmented by a secondary low pressure area created by the Ligurian sea (Genova Low). Moreover, the several and small catchments typically cause fast ooding events, and even small rivers exhibit high hydraulic energy due to the quick variation of altitude. The rain gauge network we consider here is owned by the ARPAL team of Regione Liguria, and consists of 143 professional measure stations distributed over the whole region. The measures are acquired every 5-20 minutes, and the stations are connected by GPRS and radio link connection, producing about 2 MB data per day. The resolution of the rain gauges is 0; 2mm while their accuracy is in the range of 2% error threshold.  The data are part of the dataset adopted in [2]. The dataset has been here anonymized by performing an affine transformation on the original coordinates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\mathcal{P}$ be a given point cloud that we aim to approximate through a height field $z=f(x,y)$. Let $\\mathbf{p}=(p_x,p_y)$ be a vector of positive integers. Lastly, let\n",
    "- $\\mathbf{x}=[x_1,\\ldots,x_{n_x+p_x+1}]$ be a $(p_x+1)$-regular knot vector with boundary knots $x_{p_x+1}=a_1$ and $x_{n_x+1}=b_1$.\n",
    "- $\\mathbf{y}=[y_1,\\ldots,y_{n_y+p_y+1}]$  be a $(p_y+1)$-regular knot vector with boundary knots $y_{p_y+1}=a_2$ and $y_{n_y+1}=b_2$.\n",
    "\n",
    "The *Weighted Quasi Interpolant Spline Approximation* of bi-degree $\\mathbf{p}$ to $\\mathcal{P}$ on the knot vectors $\\mathbf{x}$ and $\\mathbf{y}$ is defined by\n",
    "$$f_w(x,y):=\\sum_{i=1}^{n_x}\\sum_{j=1}^{n_y}\\hat{z}_w(x_i^\\ast,y_j^\\ast)B[x_i,\\ldots,x_{i+p_x+1};y_j,\\ldots,y_{j+p_y+1}](x,y)$$\n",
    "where\n",
    "$$\n",
    "x_i^\\ast:=\\dfrac{x_{i+1}+\\ldots+x_{i+p_x}}{p_x}, \\quad i=1,\\ldots,n_x\n",
    "$$\n",
    "and\n",
    "$$\n",
    "y_j^\\ast:=\\dfrac{y_{j+1}+\\ldots+y_{j+p_y}}{p_y}, \\quad j=1,\\ldots,n_y\n",
    "$$\n",
    "are the *knot averages* and where\n",
    "$$\n",
    "\\hat{z}(u,v):=\\dfrac{\\sum\\limits_{(x,y,z)\\in\\mathcal{P}}z\\cdot w(x,y,u,v)}{\\sum\\limits_{(x,y,z)\\in\\mathcal{P}} w(x,y,u,v)}\n",
    "$$\n",
    "is the *control points estimator* of weight function $w:\\mathbb{R}^2\\times\\mathbb{R}^2\\to[0,1]$.\n",
    "\n",
    "**In a nutshell.** We approximate a pointcloud $\\mathcal{P}$ by using a tensor mesh and estimation of the control points by weighted averages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "import math\n",
    "import statistics\n",
    "\n",
    "import LRSplines\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables initialization and parameters setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Degree along the x-coordinate:\n",
    "du = 2\n",
    "#Degree along the y-coordinate\n",
    "dv = 2\n",
    "#Cross-validation tests k-NN weights, for k=1,...,k_max:\n",
    "k_max = 15\n",
    "#Cross-validation tests meshes with n inner knots per direction, for n=1,...,n_max:\n",
    "n_max = 15 #for the sake of simplicity we consider a uniform mesh with same number of knots in each direction\n",
    "\n",
    "# Minimum of the error distribution for each choice of k and n:\n",
    "min_err = np.zeros((k_max, n_max))\n",
    "# Maximum of the error distribution for each choice of k and n:\n",
    "max_err = np.zeros((k_max, n_max))\n",
    "# Average of the error distribution for each choice of k and n:\n",
    "mean_err = np.zeros((k_max, n_max))\n",
    "# Median of the error distribution for each choice of k and n:\n",
    "median_err = np.zeros((k_max, n_max))\n",
    "# Standard deviation of the error distribution for each choice of k and n:\n",
    "std = np.zeros((k_max, n_max))\n",
    "# Mean square error of the error distribution for each choice of k and n:\n",
    "MSE = np.zeros((k_max, n_max))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We measure the predictive performances of our technique by performing 5 times a 5-fold cross-validation and then averaging the results (min, max, mean, median, std and MSE). For an exhaustive introduction on K-fold cross validation see Section 7.10.1 in [3]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For each time cross-validation is performed:\n",
    "for n_CV in range(1,6):\n",
    "    #For each couple training-validation set\n",
    "    for n_set in range(5):\n",
    "        #For each k(-NN):\n",
    "        for k in range (1, k_max + 1):\n",
    "            #For each number of inner knots:\n",
    "            for n in range(1, n_max + 1):\n",
    "        \n",
    "                #POINT CLOUDS\n",
    "                training_point_cloud = np.loadtxt(\n",
    "                    \"data/CV-\" + str(n_CV) + \"/training_point_cloud_\"+ str(n_set) + \".txt\", delimiter=',')\n",
    "                validation_point_cloud = np.loadtxt(\n",
    "                    \"data/CV-\" + str(n_CV) + \"/test_point_cloud_\"+ str(n_set) + \".txt\", delimiter=',')\n",
    "                point_cloud = np.concatenate((training_point_cloud,validation_point_cloud),axis=0)\n",
    "                \n",
    "                #Tensor mesh:\n",
    "                x_min = min(point_cloud[:,0])\n",
    "                x_max = max(point_cloud[:,0])\n",
    "                y_min = min(point_cloud[:,1])\n",
    "                y_max = max(point_cloud[:,1])\n",
    "                knots_u = [x_min, x_min] + [x_min+i*(x_max - x_min)/n for i in range(n+1)] + [x_max, x_max]\n",
    "                knots_v = [y_min, y_min] + [y_min+i*(y_max - y_min)/n for i in range(n+1)] + [y_max, y_max]\n",
    "                TP = LRSplines.init_tensor_product_LR_spline(du, dv, knots_u, knots_v)\n",
    "                \n",
    "                #Knot averages:\n",
    "                grev_x=[0 for x in range(len(TP.S))]\n",
    "                grev_y=[0 for y in range(len(TP.S))]\n",
    "                for i in range(len(TP.S)):\n",
    "                    b=TP.S[i]\n",
    "                    b_knot_x=b.knots_u\n",
    "                    b_knot_y=b.knots_v\n",
    "                    grev_x[i]=np.sum(b_knot_x[1:du+1])/du\n",
    "                    grev_y[i]=np.sum(b_knot_y[1:dv+1])/dv\n",
    "                    \n",
    "                #Coefficients of k-NN QISA\n",
    "                for l in range(len(TP.S)):\n",
    "                    #List of distances to the knot averages:\n",
    "                    local_distance = [math.sqrt(math.pow(training_point_cloud[m,0]-grev_x[l],2)+\n",
    "                                                math.pow(training_point_cloud[m,1]-grev_y[l],2)) \\\n",
    "                                      for m in range(training_point_cloud.shape[0])]\n",
    "                    #Sorted points according to the L2 distance:\n",
    "                    sorted_indexes = np.argsort(local_distance)\n",
    "                    sorted_distances = np.sort(local_distance)\n",
    "                    #k-NN to the Greville's axis:\n",
    "                    kNN = training_point_cloud[sorted_indexes[0:k]]\n",
    "                     #Average of the z-component of the k-NN set:\n",
    "                    TP.S[l].coefficient = np.sum(kNN[:,2])/k\n",
    "                    \n",
    "                #Validation error:\n",
    "                val_err = [0 for l in range(len(validation_point_cloud[:,0]))]\n",
    "                for l in range(len(validation_point_cloud[:,0])):\n",
    "                    val_err[l] = abs(TP(validation_point_cloud[l,0], validation_point_cloud[l,1])-validation_point_cloud[l,2])\n",
    "                #Centered validation error:\n",
    "                cen_val_err = [0 for l in range(len(validation_point_cloud[:,0]))]\n",
    "                for l in range(len(validation_point_cloud[:,0])):\n",
    "                    cen_val_err[l] = val_err[l] - sum(val_err) / len(val_err)\n",
    "                \n",
    "                \n",
    "                #Statistics for the error distribution of a single 5-fold cross validation.\n",
    "                #Min and max\n",
    "                min_err[k-1][n-1] = min_err[k-1][n-1] + 1/5 * min(val_err)\n",
    "                max_err[k-1][n-1] = max_err[k-1][n-1] + 1/5 * max(val_err)\n",
    "                #Mean and median\n",
    "                mean_err[k-1][n-1]   = mean_err[k-1][n-1]   + 1/5 * sum(val_err) / len(val_err)\n",
    "                median_err[k-1][n-1] = median_err[k-1][n-1] + 1/5 * statistics.median(val_err)\n",
    "                #std and MSE\n",
    "                std[k-1][n-1] = std[k-1][n-1] + 1/5 * math.sqrt(1/(len(cen_val_err)-1)*sum(map(lambda x:x*x,cen_val_err)))\n",
    "                MSE[k-1][n-1] = MSE[k-1][n-1] + 1/5 * 1/(len(val_err))*sum(map(lambda x:x*x,val_err))\n",
    "\n",
    "\n",
    "#Statistics for the error distribution when repeating 5 different times a 5-fold cross validation:\n",
    "std = std/5\n",
    "MSE = MSE/5\n",
    "min_err  = min_err/5\n",
    "max_err  = max_err/5\n",
    "mean_err = mean_err/5\n",
    "median_err = median_err/5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics for the error distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average min is    0.04710540096550549\n",
      "Average max is    2.889891020683991\n",
      "Average mean is   0.9970376033520634\n",
      "Average median is 0.9013234682760833\n",
      "Average std is    0.7082250055027819\n",
      "Average MSE is    1.5138957396858879\n"
     ]
    }
   ],
   "source": [
    "ind_min_MSE = np.unravel_index(np.argmin(MSE, axis=None), MSE.shape)\n",
    "print(\"Average min is    \" + str(min_err[ind_min_MSE]))\n",
    "print(\"Average max is    \" + str(max_err[ind_min_MSE]))\n",
    "print(\"Average mean is   \" + str(mean_err[ind_min_MSE]))\n",
    "print(\"Average median is \" + str(median_err[ind_min_MSE]))\n",
    "print(\"Average std is    \" + str(std[ind_min_MSE]))\n",
    "print(\"Average MSE is    \" + str(MSE[ind_min_MSE]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal k is 9\n",
      "Optimal n is 10\n"
     ]
    }
   ],
   "source": [
    "print(\"Optimal k is \" + str(ind_min_MSE[0]+1))\n",
    "print(\"Optimal n is \" + str(ind_min_MSE[1]+1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] A. Raffo and S. Biasotti. Weighted Quasi Interpolant Spline Approximations for Point Clouds: Properties and Applications, 2019.\n",
    "\n",
    "[2] G. Patane', A. Cerri, V. Skytt, S. Pittaluga, S. Biasotti, D. Sobrero, T. Dokken and M. Spagnuolo. Comparing Methods for the Approximation of Rainfall Fields in Environmental Applications. *ISPR Journal of Photogrammetry and Remote Sensing*, 127:57–72, 2017.\n",
    "\n",
    "[3] T. Hastie, R. Tibshirani and J. Friedman. The Elements of Statistical Learning. Data Mining, Inference, and Prediction. Springer. Second Edition. Corrected 12th printing - Jan 13, 2017. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project has received funding from the European Union’s Horizon 2020 research and innovation programme under the Marie Skłodowska-Curie grant agreement No 675789. Dott. S. Biasotti work has been partially supported by the EU ERC Advanced Grant CHANGE, grant agreement No. 694515 and the CNR-IMATI project DIT.AD021.080.001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
